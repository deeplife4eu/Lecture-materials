{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 14 - Deep Learning for Image Segmentation: U-Net\n",
    "\n",
    "## Biomedical Computer Vision Group (BMCV) <br> BioQuant, IPMB, Heidelberg University\n",
    "\n",
    "Image segmentation is an important task for biomedical image analysis enabling many downstream tasks, such as cell counting or tracking. Image segmentation assigns a specific class to each pixel of an image. In recent years, deep learning based methods often yield better segmentation results compared to classical methods.\n",
    "\n",
    "Fully convolutional neural networks enable classifying all pixels of entire images in a single forward pass. The [U-Net Architecture](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28) (Ronneberger et al., MICCAI, 2015) builds upon this idea and was proposed for segmentation of biomedical images. Even today, the U-Net and its extensions are still widely used in many applications and often yield state-of-the-art performances.\n",
    "\n",
    "This notebook comprises the following sections:\n",
    "\n",
    "1) [Loading and Visualizing the Data](#1---data)\n",
    "2) [Implementing the U-Net](#2---2d-u-net)\n",
    "3) [Data Slicing and Augmentation](#3---data-slicing-and-augmentation)\n",
    "4) [Training, Validation and Testing](#4---training-validation-and-testing)\n",
    "\n",
    "Here you can jump to the exercises:\n",
    "\n",
    "[Exercise 1](#exercise-1-u-net-decoder), \n",
    "[Exercise 2](#exercise-2),\n",
    "[Exercise 3](#exercise-3), \n",
    "[Exercise 4](#exercise-4), \n",
    "[Exercise 5](#exercise-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Import Packages and Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "import scipy.ndimage\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from skimage.transform import resize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting random seeds for reproducable results\n",
    "random_seed = 1\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#Setting the device on which the U-Net runs\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Network will run on \"{device}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(*args):\n",
    "    f,ax = plt.subplots(1,len(args), figsize = (20,20))\n",
    "    for i in range(len(args)):\n",
    "        ax[i].imshow(args[i][0], cmap='gray')\n",
    "        ax[i].set_title(args[i][1])\n",
    "        ax[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def plot_results(*args):\n",
    "    for i in range(len(args)):\n",
    "        plt.plot(args[i][0], label=args[i][1])\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data\n",
    "\n",
    "The data required for this notebook show HeLa cells on a flat glass acquired with differential interference contrast (DIC) microscopy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "def create_url(mode, dataset):\n",
    "    \"\"\"\n",
    "    Construct URL of CTC dataset.\n",
    "\n",
    "    CTC datasets have a train and test dataset for each image sequence.\n",
    "    \"\"\"\n",
    "    m = mode + \"ing\" if mode == \"train\" else mode\n",
    "    return f\"http://data.celltrackingchallenge.net/{m}-datasets/{dataset}.zip\"\n",
    "\n",
    "def download_dataset(base_dir, dataset_name, mode=\"train\"):\n",
    "    \"\"\"\n",
    "    Download and extract CTC dataset.\n",
    "    \"\"\"\n",
    "    # setup paths\n",
    "    data_dir = base_dir / mode\n",
    "    dataset_dir = data_dir / dataset_name\n",
    "\n",
    "    # check if data was already downloaded\n",
    "    if dataset_dir.exists():\n",
    "        print(f\"Dataset {dataset_name} already exists.\")\n",
    "    else:\n",
    "        # if data was not downloaded, yet, create folder it will be saved to and CTC url\n",
    "        dataset_dir.mkdir(parents=True)\n",
    "        download_url = create_url(mode, dataset_name)\n",
    "\n",
    "        # download data and check that URL actually exists\n",
    "        print(f\"Downloading {mode} data from {dataset_name} ({download_url}) to {dataset_dir}/{dataset_name}.\")\n",
    "        r = requests.get(download_url)\n",
    "        assert r.status_code == 200\n",
    "\n",
    "        # extract zip file to data folder\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall(data_dir)\n",
    "\n",
    "    print(\"Download finished.\")\n",
    "    return dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"exercise_data\")\n",
    "dataset_name = \"DIC-C2DH-HeLa\"\n",
    "\n",
    "# download_dataset returns the name of the directory, the data was downloaded to\n",
    "dataset_dir = download_dataset(base_dir, dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_frames(frames, n, random_seed):\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    return tuple(rng.choice(frames, n, replace=False))\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    dataset_dir, sequence, suffix=\"ST\", n=None, random_seed=11, min_cells=0, ignore = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load images and masks from a dataset directory.\n",
    "\n",
    "    Loads n randomly selected images from a given sequence of a CTC dataset.\n",
    "    \"\"\"\n",
    "    # create directory names\n",
    "    img_dir = dataset_dir / sequence\n",
    "    msk_dir = dataset_dir / f\"{sequence}_{suffix}\" / \"SEG\"\n",
    "    if suffix == \"ERR_SEG\":\n",
    "        msk_dir = msk_dir.parent\n",
    "    # find out which frames have provided masks\n",
    "    min_cells = min_cells or 0\n",
    "    ignore = ignore or []\n",
    "    frames = [\n",
    "        re.search(r\"\\d{3}$\", fp.stem).group()\n",
    "        for fp in sorted(msk_dir.glob(\"*.tif\"))\n",
    "        if len(np.unique(skimage.io.imread(fp))) > min_cells\n",
    "    ]\n",
    "\n",
    "    if ignore is not None:\n",
    "        frames = np.setdiff1d(frames, ignore)\n",
    "\n",
    "    # use all available frames or randomly sample n frames that should be used/loaded\n",
    "    frames_sampled = (\n",
    "        tuple(frames)\n",
    "        if (n is None) or (n > len(frames))\n",
    "        else sample_frames(frames, n, random_seed)\n",
    "    )\n",
    "\n",
    "    # print info on the data that will be loaded\n",
    "    if (n is not None) and (n > len(frames)):\n",
    "        print(\n",
    "            f\"Number of requested frames ({n}) exceeds the number of frames in sequence ({len(frames)}). Loading all {len(frames)} frames.\"\n",
    "        )\n",
    "    elif n is not None:\n",
    "        print(\n",
    "            f\"Loading {len(frames_sampled)} / {len(frames)} images from sequence {sequence} of {dataset_dir.name}.\"\n",
    "        )\n",
    "        print(f\"Loaded frames {sorted(frames_sampled)}\")\n",
    "\n",
    "    # load images and masks\n",
    "    images = [\n",
    "        skimage.io.imread(fp)\n",
    "        for fp in sorted(img_dir.glob(\"*.tif\"))\n",
    "        if fp.stem.endswith(frames_sampled)\n",
    "    ]\n",
    "    masks = [\n",
    "        skimage.io.imread(fp)\n",
    "        for fp in sorted(msk_dir.glob(\"*.tif\"))\n",
    "        if fp.stem.endswith(frames_sampled)\n",
    "    ]\n",
    "\n",
    "    return np.stack(images), np.stack(masks), frames_sampled\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val, frames_val = create_dataset(dataset_dir, \"01\", suffix=\"ST\", n = 5)\n",
    "x_train, y_train, _ = create_dataset(dataset_dir, \"01\", suffix=\"ST\", ignore = frames_val, n = 10)\n",
    "x_test, y_test,_ = create_dataset(dataset_dir, \"02\", suffix=\"GT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(0)\n",
    "colors = [(0,0,0)] + [(random_state.random(),random_state.random(),random_state.random()) for _ in range(len(np.unique(y_train[0])))]\n",
    "random_cmap = matplotlib.colors.LinearSegmentedColormap.from_list('random_cmap', colors)\n",
    "\n",
    "\n",
    "f,ax = plt.subplots(1,2, figsize = (20,20))\n",
    "ax[0].imshow(x_train[0], cmap = 'gray')\n",
    "ax[0].set_title('Raw Image')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(y_train[0], cmap = random_cmap,  interpolation = 'nearest')\n",
    "ax[1].set_title('Ground Truth')\n",
    "ax[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - 2D U-Net\n",
    "\n",
    "The U-Net Architecture has an encoder-decoder structure with long range skip-connection connecting the encoder with the decoder. In this section, first we will construct the individual layers required to finally construct the whole U-Net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 U-Net Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, kernel_size, padding, pooling, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        if pooling:\n",
    "            self.pooling = nn.MaxPool2d(2)\n",
    "        else:\n",
    "            self.pooling = None\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout) if dropout > 0 else nn.Identity(),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        if self.pooling is not None:\n",
    "            x = self.pooling(x)\n",
    "        x = self.block(x)\n",
    "        return(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: U-Net Decoder\n",
    "\n",
    "Below you will find the decoder class. As you have learned in the lecture, there are different options for implementing the skip-connections. The code for the skip-connections by concatenation is already given. Based on this, it is now your task to complete missing code for\n",
    "\n",
    "1) additive skip-connections\n",
    "2) no skip-connections\n",
    "\n",
    "The places where code is to be inserted is marked with `# your code goes here` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, kernel_size, padding, dropout, skip_mode='concat', upsampling_mode='transpose', cropping=False):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        assert upsampling_mode in ['transpose', 'interpolate'], f'Upsampling has to be either \"transpose\" or \"interpolate\" but got \"{upsampling_mode}\"'\n",
    "        assert skip_mode in ['concat', 'add', 'none'], f'Skip-connection has to be either \"none\", \"add\" or \"concat\" but got \"{skip_mode}\"'\n",
    "\n",
    "        self.cropping = cropping\n",
    "        self.skip_mode = skip_mode\n",
    "        self.upsampling_mode = upsampling_mode\n",
    "\n",
    "        if self.upsampling_mode == 'transpose':\n",
    "            self.up = nn.ConvTranspose2d(ch_in, ch_out, kernel_size=2, stride = 2)\n",
    "        else:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            #1x1 convolution equalizes the number of channels\n",
    "            self.conv = nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        if self.skip_mode == 'concat':\n",
    "            ch_hidden = ch_out + ch_out\n",
    "        elif self.skip_mode == 'add':\n",
    "            ch_hidden = ch_out\n",
    "        elif self.skip_mode == 'none':\n",
    "            ch_hidden = ch_out\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(ch_hidden, ch_out, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout) if dropout > 0 else nn.Identity(),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def crop(self, x, cropping_size):\n",
    "        return(x[:,:,cropping_size[0]:-cropping_size[0], cropping_size[1]:-cropping_size[1]])\n",
    "\n",
    "    def forward(self,x, skip_features):\n",
    "        if self.upsampling_mode == 'transpose':\n",
    "            x = self.up(x)\n",
    "        elif self.upsampling_mode == 'interpolate':\n",
    "            x = self.up(x)\n",
    "            x = self.conv(x)\n",
    "\n",
    "        if self.cropping:\n",
    "            cropping_size = (torch.tensor(skip_features.shape[2:]) - torch.tensor(x.shape[2:]))//2\n",
    "            skip_features = self.crop(skip_features, cropping_size)\n",
    "            \n",
    "        if self.skip_mode == 'concat':\n",
    "            x = self.block(torch.cat((x, skip_features), 1))\n",
    "        elif self.skip_mode == 'add':\n",
    "            # your code goes here\n",
    "            pass\n",
    "        elif self.skip_mode == 'none':\n",
    "            # your code goes here\n",
    "            pass\n",
    "        \n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 U-Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet2d(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 encoder_layer=EncoderLayer,\n",
    "                 decoder_layer=DecoderLayer,\n",
    "                 hidden_dims=[64,128,256,512,1024], \n",
    "                 kernel_size=3,\n",
    "                 padding_mode='valid', \n",
    "                 skip_mode='concat', \n",
    "                 upsampling_mode='transpose', \n",
    "                 dropout=0,\n",
    "                 ):\n",
    "        \n",
    "        super(UNet2d, self).__init__()\n",
    "\n",
    "        assert len(hidden_dims) > 0, 'UNet2d requires at least one hidden layer'\n",
    "        assert padding_mode in ['same', 'valid'], f'Padding mode has to be either \"same\" or \"valid\" but got \"{padding_mode}\"'\n",
    "\n",
    "        self.padding_mode = padding_mode\n",
    "        \n",
    "        cropping = True if padding_mode == 'valid' else False\n",
    "        padding = 0 if padding_mode == 'valid' else kernel_size//2\n",
    "\n",
    "        #Assembling the encoder\n",
    "        encoder = []\n",
    "        for i in range(len(hidden_dims)):\n",
    "            if i == 0:\n",
    "                ch_in = input_dim\n",
    "                ch_out = hidden_dims[i]\n",
    "                encoder.append(encoder_layer(ch_in, ch_out, kernel_size=kernel_size, padding=padding, pooling=False, dropout=0))\n",
    "            elif i == (len(hidden_dims) - 1):\n",
    "                ch_in = hidden_dims[i-1]\n",
    "                ch_out = hidden_dims[i]\n",
    "                encoder.append(encoder_layer(ch_in, ch_out, kernel_size=kernel_size, padding=padding, pooling=True, dropout=dropout))\n",
    "            else:\n",
    "                ch_in = hidden_dims[i-1]\n",
    "                ch_out = hidden_dims[i]\n",
    "                encoder.append(encoder_layer(ch_in, ch_out, kernel_size=kernel_size, padding=padding, pooling=True, dropout=0))\n",
    "        self.encoder = nn.ModuleList(encoder)\n",
    "\n",
    "        #Assembling the decoder\n",
    "        decoder = []\n",
    "\n",
    "        #Reversing the order of the hidden dims, since the decoder reduces the number of channels\n",
    "        hidden_dims_rev = hidden_dims[::-1]\n",
    "\n",
    "        for i in range(len(hidden_dims_rev) - 1):\n",
    "            ch_in = hidden_dims_rev[i]\n",
    "            ch_out = hidden_dims_rev[i+1]\n",
    "            decoder.append(decoder_layer(ch_in, ch_out, kernel_size=kernel_size, padding=padding, dropout=0, skip_mode=skip_mode, upsampling_mode=upsampling_mode, cropping=cropping))\n",
    "        self.decoder = nn.ModuleList(decoder)\n",
    "\n",
    "        #Creating final 1x1 convolution \n",
    "        self.final_conv = nn.Conv2d(hidden_dims[0], output_dim, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Forward pass of the encoder\n",
    "        skip_features = []\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x)\n",
    "            skip_features.insert(0, x)\n",
    "\n",
    "        #Removing bottleneck features from the feature list\n",
    "        skip_features = skip_features[1:]\n",
    "\n",
    "        #Forward pass of the decoder\n",
    "        for i, decoder_layer in enumerate(self.decoder):\n",
    "            skip = skip_features[i]\n",
    "            x = decoder_layer(x, skip)\n",
    "\n",
    "        #Performing the final 1x1 convolution\n",
    "        x = self.final_conv(x)\n",
    "        return(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "    if isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Data Slicing and Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Data Slicing\n",
    "\n",
    "In order to process arbitrarily large images, they can be sliced into overlapping patches smaller than the image itself. This sliding window approach reduces the computational load, since we do not need to process the whole image at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slices(dataset_shape, patch_size, stride):\n",
    "    x, y = dataset_shape\n",
    "    p_x, p_y = patch_size\n",
    "    s_x, s_y = stride\n",
    "\n",
    "    #Generate indices in x-direction\n",
    "    x_indices = np.arange(0, x - p_x + 1, s_x)\n",
    "    if x_indices[-1] + p_x < x:\n",
    "        x_indices = np.append(x_indices, x - p_x)\n",
    "\n",
    "    #Generate indices in y-direction\n",
    "    y_indices = np.arange(0, y - p_y + 1, s_y)\n",
    "    if y_indices[-1] + p_y < y:\n",
    "        y_indices = np.append(y_indices, y - p_y)\n",
    "\n",
    "    slices = []\n",
    "    for idx_x in x_indices:\n",
    "        for idx_y in y_indices:\n",
    "            slices.append((\n",
    "                slice(idx_x, idx_x + p_x),\n",
    "                slice(idx_y, idx_y + p_y)\n",
    "            ))\n",
    "    return(slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Data Transforms\n",
    "\n",
    "In order to bring the data into a format suited for pytorch, we need to define transformations which are called when loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToFloat():\n",
    "    def __call__(self, x):\n",
    "        return(x.astype(np.float32))\n",
    "    \n",
    "class ToTensor():\n",
    "    def __call__(self, x):\n",
    "        if len(x.shape) == 2:\n",
    "            return(torch.from_numpy(x.copy()).unsqueeze(0))\n",
    "        elif len(x.shape) == 3:\n",
    "            return(torch.from_numpy(x.copy()))\n",
    "        \n",
    "\n",
    "class Normalize():\n",
    "    def __init__(self, min=None, max =None, eps=1e-8):\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "        self.eps = eps\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.min is not None and self.max is not None:\n",
    "            return((x - self.min) / (self.max - self.min + self.eps))\n",
    "        else:\n",
    "            return((x - x.min()) / (x.max() - x.min() + self.eps))\n",
    "        \n",
    "\n",
    "#When visualizing the data, we saw that the \n",
    "#ground truth contains instance segmentation masks.\n",
    "#This class transforms them into a binary mask\n",
    "#which can be used for foreground/background segmentation\n",
    "class Instance2Semantic():\n",
    "    def __call__(self, x):\n",
    "        return((x > 0).astype(int))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Data Augmentation\n",
    "\n",
    "Neural networks require large annotated datasets in order to perform well. However, acquiring annotated datasets for biomedical image segmentation typically requires expert knowledge and is therefore costly and time-consuming. One way to cope with the lack of annotated datasets is to artificially increase the dataset by augmenting the existing labeled samples. In this section, multiple classes for augmenting images during training will be introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRotate90():\n",
    "    def __init__(self, random_state, execution_probability=0.0):\n",
    "        self.random_state = random_state\n",
    "        self.execution_probability = execution_probability\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.random_state.random() < self.execution_probability:\n",
    "            k = self.random_state.randint(1,4)\n",
    "            return(np.rot90(x,k))\n",
    "        return(x)\n",
    "\n",
    "class RandomContrast():\n",
    "    def __init__(self, random_state, scale, execution_probability=0.0):\n",
    "        self.random_state = random_state\n",
    "        self.execution_probability = execution_probability\n",
    "        self.scale = scale\n",
    "    def __call__(self, x):\n",
    "        if self.random_state.random() < self.execution_probability:\n",
    "            lam = self.random_state.uniform(np.max((0, 1 - self.scale)), 1 + self.scale)\n",
    "            return(np.clip(x * lam, 0, 255))\n",
    "        return(x)\n",
    "\n",
    "class RandomBrightness():\n",
    "    def __init__(self,random_state, scale, execution_probability=0.0):\n",
    "        self.random_state = random_state\n",
    "        self.execution_probability = execution_probability\n",
    "        self.scale = scale\n",
    "    def __call__(self, x):\n",
    "        if self.random_state.random() < self.execution_probability:\n",
    "            lam = 255 * self.scale * self.random_state.uniform(-1,1)\n",
    "            return(np.clip(x + lam, 0, 255))\n",
    "        return(x)\n",
    "    \n",
    "class RandomElasticDeformation():\n",
    "    def __init__(self, random_state, grid_size, sigma, order, execution_probability=0.0):\n",
    "        self.random_state = random_state\n",
    "        self.grid_size = grid_size\n",
    "        self.sigma = sigma\n",
    "        self.order = order\n",
    "        self.execution_probability = execution_probability\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.random_state.random() < self.execution_probability:\n",
    "            h,w = x.shape\n",
    "            \n",
    "            dx = self.random_state.randn(*[self.grid_size, self.grid_size]) * self.sigma\n",
    "            dy = self.random_state.randn(*[self.grid_size, self.grid_size]) * self.sigma\n",
    "\n",
    "            dx = resize(dx, x.shape, preserve_range=True, order = 3)\n",
    "            dy = resize(dy, x.shape, preserve_range=True, order = 3)\n",
    "\n",
    "            idx_x, idx_y = np.meshgrid(np.arange(h), np.arange(w))\n",
    "\n",
    "            coordinates = idx_y + dy, idx_x + dx\n",
    "\n",
    "            return(scipy.ndimage.map_coordinates(x, coordinates, order = self.order, mode = 'reflect', prefilter=False))\n",
    "        return(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = x_train[0]\n",
    "\n",
    "rot90 = RandomRotate90(random_state=np.random.RandomState(random_seed), execution_probability=1)(img)\n",
    "contrast = RandomContrast(random_state=np.random.RandomState(random_seed), scale=0.5, execution_probability=1)(img)\n",
    "brightness = RandomBrightness(random_state=np.random.RandomState(random_seed), scale=1, execution_probability=1)(img)\n",
    "deform = RandomElasticDeformation(random_state=np.random.RandomState(random_seed), grid_size=3, sigma=25, order=3, execution_probability=1)(img)\n",
    "\n",
    "plot_images(\n",
    "    [img, 'Original image'],\n",
    "    [rot90, 'Random 90Â° rotation'],\n",
    "    [contrast, 'Random contrast'],\n",
    "    [brightness, 'Random brightness'],\n",
    "    [deform, 'Random elastic deformation']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Based on the existing classes for data augmentation, write a class for randomly flipping an image vertically or horizontally with a certain execution probability. Test your implementation by applying it to an image from the training dataset. Use the provided templates. \n",
    "\n",
    "(BONUS) Additionally, implement a class for randomly rotating an image in a pre-defined angle range.\n",
    "\n",
    "HINT: You can use pre-implemented function like `np.flip` and `scipy.ndimage.rotate`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomFlip():\n",
    "    def __init__(self, random_state, execution_probability=0.0):\n",
    "        self.random_state = random_state\n",
    "        self.execution_probability = execution_probability\n",
    "    def __call__(self, x):\n",
    "        if self.random_state.random() < self.execution_probability:\n",
    "            # your code goes here\n",
    "            pass\n",
    "        return(x)\n",
    "\n",
    "class RandomRotate():\n",
    "    def __init__(self, random_state, angle_range, execution_probability=0.0, mode='mirror', order=0, reshape=False):\n",
    "        self.random_state = random_state\n",
    "        self.angle_range = angle_range #This is a list specifing the angle range, i.e. [angle_min, angle_max]\n",
    "        self.execution_probability = execution_probability\n",
    "        self.mode = mode\n",
    "        self.order = order\n",
    "        self.reshape = reshape\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.random_state.random() < self.execution_probability:\n",
    "            # your code goes here\n",
    "            pass\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip = RandomFlip(random_state=np.random.RandomState(random_seed), execution_probability=1)(img)\n",
    "rot = RandomRotate(random_state=np.random.RandomState(random_seed), angle_range=[-45,45], execution_probability=1, mode = 'constant')(img)\n",
    "\n",
    "plot_images(\n",
    "    [img, 'Original image'],\n",
    "    [flip, 'Random horizontal/vertical flip'],\n",
    "    [rot, 'Random rotation']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Training, Validation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Dataset\n",
    "\n",
    "Next we need to define the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 image,\n",
    "                 label,\n",
    "                 patch_size, \n",
    "                 stride, \n",
    "                 padding,\n",
    "                 image_transformer,\n",
    "                 label_transformer,\n",
    "                 phase,\n",
    "                 ):\n",
    "        \n",
    "\n",
    "        self.phase = phase\n",
    "\n",
    "        self.image_transformer = image_transformer\n",
    "        self.label_transformer = label_transformer\n",
    "\n",
    "        self.image = image\n",
    "        self.label = label\n",
    "\n",
    "        self.image_shape = self.image.shape\n",
    "        if padding is not None:\n",
    "            self.image = np.pad(self.image, pad_width=((padding[0], padding[0]), (padding[1], padding[1])), mode='reflect')\n",
    "            self.label = np.pad(self.label, pad_width=((padding[0], padding[0]), (padding[1], padding[1])), mode='reflect')\n",
    "\n",
    "        self.slices = get_slices(self.image.shape, patch_size, stride)\n",
    "\n",
    "\n",
    "    def __len__(self,):\n",
    "        return(len(self.slices))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sl = self.slices[index]\n",
    "        \n",
    "        x = self.image[sl]\n",
    "        y = self.label[sl]\n",
    "        \n",
    "        x = self.image_transformer(x)\n",
    "        y = self.label_transformer(y)\n",
    "\n",
    "        if self.phase == 'test':\n",
    "            return(x, y, sl)\n",
    "        else:\n",
    "            return(x, y)\n",
    "        \n",
    "#This function is needed to form the batch from a list of samples\n",
    "def custom_collate(batch):\n",
    "    if isinstance(batch[0], torch.Tensor):\n",
    "        return(torch.stack(batch, 0))\n",
    "    elif isinstance(batch[0], slice):\n",
    "        return(batch)\n",
    "    elif isinstance(batch[0], tuple):\n",
    "        return([custom_collate(obj) for obj in zip(*batch)])\n",
    "\n",
    "        \n",
    "\n",
    "def get_dataloader(x, y, batch_size, patch_size, stride, padding, image_transforms, label_transforms, phase):\n",
    "    assert phase in ['train', 'validate', 'test'], f'Phase has to be either \"train\", \"validate\" or \"test\" but got \"{phase}\"'\n",
    "    shuffle = True if phase == 'train' else False\n",
    "    data = []\n",
    "    for i in range(x.shape[0]):\n",
    "        dataset = UNetDataset(x[i], y[i], patch_size, stride,padding, image_transforms, label_transforms, phase)\n",
    "        data.append(dataset)\n",
    "    dataloader = DataLoader(dataset=ConcatDataset(data), batch_size=batch_size, shuffle=shuffle, num_workers=0, collate_fn=custom_collate)\n",
    "    return(dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 - Evaluation Metrics\n",
    "\n",
    "For evaluation of the U-Net, we will use the Dice or IoU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dice(pred, gt):\n",
    "    numerator = 2 * np.sum(pred * gt, axis = (1,2))\n",
    "    denominator = np.sum(pred, axis = (1,2)) + np.sum(gt, axis = (1,2))\n",
    "    return(np.mean(numerator / denominator))\n",
    "\n",
    "def IoU(pred, gt):\n",
    "    numerator = np.sum(pred * gt, axis = (1,2))\n",
    "    denominator = np.sum(pred, axis = (1,2)) + np.sum(gt, axis = (1,2)) - np.sum(pred * gt, axis = (1,2))\n",
    "    return(np.mean(numerator / denominator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 - Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetTrainer():\n",
    "    def __init__(self, model, optimizer, lr_scheduler, loss_function, device):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.loss_function = loss_function\n",
    "        self.device = device\n",
    "        self.total_epochs = 0\n",
    "        self.total_iterations = 0\n",
    "\n",
    "        self.training_losses = []\n",
    "        self.validation_losses = []\n",
    "\n",
    "\n",
    "    def train(self, epochs, dataloader_train, dataloader_val):\n",
    "        print('Starting training...')\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            training_loss = 0\n",
    "            for data in dataloader_train:\n",
    "                input, target = data\n",
    "\n",
    "                input = input.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "\n",
    "                output = self.model(input)\n",
    "\n",
    "                if self.model.padding_mode == 'valid':\n",
    "                    cropping_size = (torch.tensor(input.shape[2:]) - torch.tensor(output.shape[2:]))//2\n",
    "                    target = target[:,:,cropping_size[0]:-cropping_size[0], cropping_size[1]:-cropping_size[1]]\n",
    "\n",
    "\n",
    "                loss = self.loss_function(output, target[:,0].long()) \n",
    "\n",
    "                training_loss += loss.item() \n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()     \n",
    "\n",
    "                self.total_iterations += 1\n",
    "\n",
    "            \n",
    "            validation_loss = self.validate(dataloader_val)\n",
    "\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step(validation_loss)\n",
    "                \n",
    "            print(f'Epoch: [{epoch + 1}/{epochs}],', \n",
    "                  f'Total iterations: {self.total_iterations},', \n",
    "                  f'Training Loss: {training_loss / len(dataloader_train)},',\n",
    "                  f'Validation Loss: {validation_loss/ len(dataloader_val)}',)\n",
    "\n",
    "            self.total_epochs += 1\n",
    "\n",
    "            self.training_losses.append(training_loss / len(dataloader_train))\n",
    "            self.validation_losses.append(validation_loss / len(dataloader_val)) \n",
    "\n",
    "    def validate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        validation_loss = 0\n",
    "        for data in dataloader:\n",
    "            input, target = data\n",
    "\n",
    "            input = input.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "\n",
    "            output = self.model(input)\n",
    "\n",
    "            if self.model.padding_mode == 'valid':\n",
    "                cropping_size = (torch.tensor(input.shape[2:]) - torch.tensor(output.shape[2:]))//2\n",
    "                target = target[:,:,cropping_size[0]:-cropping_size[0], cropping_size[1]:-cropping_size[1]]\n",
    "\n",
    "            loss = self.loss_function(output, target[:,0].long())\n",
    "\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "        self.model.train()\n",
    "        return(validation_loss)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpad(img, padding_size):\n",
    "    if len(img.shape) == 3:\n",
    "        return(img[:,padding_size[0]:-padding_size[0], padding_size[1]:-padding_size[1]])\n",
    "    else:\n",
    "        return(img[padding_size[0]:-padding_size[0], padding_size[1]:-padding_size[1]])\n",
    "\n",
    "def predict(model, x_test, y_test, patch_size, stride, padding, image_transforms, label_transforms, eval_metric = None):\n",
    "    dataloader = get_dataloader(x=x_test[None], y=y_test[None], batch_size=1, patch_size=patch_size, stride=stride, padding=padding, image_transforms=image_transforms, label_transforms=label_transforms, phase='test')\n",
    "    image_size = dataloader.dataset.datasets[0].image_shape\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    padded_shape = (image_size[0] + 2 * padding[0], image_size[1] + 2 * padding[1])\n",
    "    prediction = np.zeros((2,) + padded_shape)\n",
    "    normalization = np.zeros(padded_shape)\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            input, _, current_patch_index = data\n",
    "\n",
    "            input = input.to(device)\n",
    "\n",
    "            output = model(input)\n",
    "\n",
    "\n",
    "            for b in range(input.shape[0]):\n",
    "                if input.shape[2] == output.shape[2] and input.shape[3] == output.shape[3]:\n",
    "                    normalization[(current_patch_index[0][b], current_patch_index[1][b])] += 1\n",
    "                    prediction[(slice(0,2,None),) + (current_patch_index[0][b], current_patch_index[1][b])] += output[b].detach().cpu().numpy()\n",
    "\n",
    "                else:\n",
    "                    cropping_size = (torch.tensor(input.shape[2:]) - torch.tensor(output.shape[2:]))//2 \n",
    "                    padded_index = (\n",
    "                        slice(current_patch_index[0][b].start + cropping_size[0], current_patch_index[0][b].stop - cropping_size[0]),\n",
    "                        slice(current_patch_index[1][b].start + cropping_size[1], current_patch_index[1][b].stop - cropping_size[1])\n",
    "                    )\n",
    "                    normalization[padded_index] += 1\n",
    "                    prediction[(slice(0,2,None),) + padded_index] += output[b].detach().cpu().numpy()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    if padding[0] != 0 and padding[1] != 0:\n",
    "        prediction = unpad(prediction, padding)\n",
    "        normalization = unpad(normalization, padding)\n",
    "\n",
    "    result = (prediction / normalization).argmax(0)\n",
    "\n",
    "    if eval_metric is not None:\n",
    "        eval_score = eval_metric(result[None], (y_test>0)[None])\n",
    "    else:\n",
    "        eval_score = None\n",
    "        \n",
    "    return(result, eval_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the transformations and augmentations for the training images and labels. \n",
    "execution_probability = 0.1 #Probability that augmentations will be applied to the images\n",
    "image_transforms_train = Compose([\n",
    "    ToFloat(),\n",
    "    RandomRotate90(random_state=np.random.RandomState(random_seed), execution_probability=execution_probability),\n",
    "    RandomRotate(random_state=np.random.RandomState(random_seed), angle_range=[-10,10], execution_probability=execution_probability, order=2),\n",
    "    RandomFlip(random_state=np.random.RandomState(random_seed), execution_probability=execution_probability),\n",
    "    RandomElasticDeformation(random_state=np.random.RandomState(random_seed), grid_size=3, sigma=10, order=3, execution_probability=execution_probability),\n",
    "    RandomContrast(random_state=np.random.RandomState(random_seed), scale=0.1, execution_probability=execution_probability),\n",
    "    RandomBrightness(random_state=np.random.RandomState(random_seed), scale=0.1, execution_probability=execution_probability),\n",
    "    Normalize(min=0, max=255),\n",
    "    ToTensor()\n",
    "])\n",
    "label_transforms_train = Compose([\n",
    "    Instance2Semantic(),\n",
    "    RandomRotate90(random_state=np.random.RandomState(random_seed), execution_probability=execution_probability),\n",
    "    RandomRotate(random_state=np.random.RandomState(random_seed), angle_range=[-10,10], execution_probability=execution_probability, order=0),\n",
    "    RandomFlip(random_state=np.random.RandomState(random_seed), execution_probability=execution_probability),\n",
    "    RandomElasticDeformation(random_state=np.random.RandomState(random_seed), grid_size=3, sigma=10, order=1, execution_probability=execution_probability),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#Defining the transformations and augmentations for the validation images and labels. \n",
    "image_transforms_val = Compose([\n",
    "    ToFloat(),\n",
    "    Normalize(min=0, max=255),\n",
    "    ToTensor()\n",
    "])\n",
    "label_transforms_val = Compose([\n",
    "    Instance2Semantic(),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#Defining the transformations and augmentations for the test images and labels. \n",
    "image_transforms_test = Compose([\n",
    "    ToFloat(),\n",
    "    Normalize(min=0, max=255),\n",
    "    ToTensor()\n",
    "])\n",
    "label_transforms_test = Compose([\n",
    "    Instance2Semantic(),\n",
    "    ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training of the original U-Net\n",
    "\n",
    "#Network parameters\n",
    "input_channel = 1\n",
    "output_channel = 2\n",
    "hidden_dims = [32,64,128,256,512]\n",
    "kernel_size = 3                         \n",
    "padding_mode = 'valid'                  # possible padding modes: 'same', 'valid'\n",
    "skip_mode = 'concat'                    # possible skip modes: 'none', 'add', 'concat'\n",
    "upsampling_mode = 'transpose'           # possible upsampling modes: 'interpolate', 'transpose'\n",
    "dropout = 0.1                           # note: dropout is only applied to the bottleneck features\n",
    "encoder_layer = EncoderLayer            \n",
    "decoder_layer = DecoderLayer            \n",
    "\n",
    "#Dataset parameters\n",
    "batch_size = 4                         \n",
    "patch_size_original_train = [332,332]   # patches cannot be larger than the image itself        \n",
    "stride_original_train = [128,128]       # stride should be smaller than the patches \n",
    "padding_original_train = [0,0]          # when using 'valid' convolution, padding can be used to cope with the border problem\n",
    "\n",
    "#Training parameters\n",
    "loss_function = nn.CrossEntropyLoss()   \n",
    "eval_metric = Dice\n",
    "learning_rate = 1e-3\n",
    "\n",
    "#Preparing dataloader\n",
    "dataloader_train = get_dataloader(x=x_train, y=y_train, batch_size=batch_size, patch_size=patch_size_original_train, stride=stride_original_train, padding=padding_original_train, image_transforms=image_transforms_train, label_transforms=label_transforms_train, phase='train')\n",
    "dataloader_val = get_dataloader(x=x_val, y=y_val, batch_size=batch_size, patch_size=patch_size_original_train, stride=stride_original_train, padding=padding_original_train, image_transforms=image_transforms_val, label_transforms=label_transforms_val, phase='validate')\n",
    "\n",
    "#Initializing the U-Net\n",
    "unet = UNet2d(input_channel,output_channel,encoder_layer,decoder_layer, hidden_dims, kernel_size, padding_mode, skip_mode, upsampling_mode, dropout)\n",
    "unet.apply(weights_init)\n",
    "optimizer = optim.SGD(unet.parameters(), lr=learning_rate, momentum=0.99)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=50, verbose=True)\n",
    "\n",
    "trainer_original = UNetTrainer(unet, optimizer, lr_scheduler, loss_function, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "trainer_original.train(epochs, dataloader_train, dataloader_val)\n",
    "\n",
    "plot_results(\n",
    "    [trainer_original.training_losses, 'Training loss'],\n",
    "    [trainer_original.validation_losses, 'Validation loss']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size_original_test= [332,332]\n",
    "stride_original_test = [128,128]\n",
    "padding_original_test = [92,92]\n",
    "\n",
    "test_scores_original = []\n",
    "\n",
    "for t in range(len(x_test)):\n",
    "    prediction, dice_score = predict(trainer_original.model, x_test[t], y_test[t], patch_size=patch_size_original_test, stride=stride_original_test, padding=padding_original_test, image_transforms = image_transforms_test, label_transforms=label_transforms_test, eval_metric=eval_metric)\n",
    "    test_scores_original.append(dice_score)\n",
    "\n",
    "print(f'Mean dice score for the test dataset using the original U-Net: {np.mean(test_scores_original)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "prediction_original, dice_score = predict(trainer_original.model, x_test[idx], y_test[idx], patch_size=patch_size_original_test, stride=stride_original_test, padding=padding_original_test, image_transforms = image_transforms_test, label_transforms=label_transforms_test, eval_metric=eval_metric)\n",
    "\n",
    "plot_images(\n",
    "    [x_test[idx], 'Original image'],\n",
    "    [prediction_original, f'Prediction original U-Net \\n Dice = {np.round(dice_score,4)}'],\n",
    "    [y_test[idx]>0, 'Ground truth']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "The U-Net we have introduced so far is very much based on the original U-Net proposed in 2015. There are several options to improve the performance of the network. Try the following steps. You can either perform each step at a time to see the improvements individually, or change everything at once. \n",
    "\n",
    "1) Add batch normalization layers to the encoder and decoder (since we are using a rather small training dataset, set `track_running_stats=False` for improved results. This way the statistics for the batch normalization are calculated for each batch individually)\n",
    "2) For convolution use 'same' instead of 'valid'\n",
    "3) Replace the transposed convolution with bilinear interpolation\n",
    "4) Replace SGD optimizer with a more advanced optimizer like Adam \n",
    "\n",
    "Report your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayerBN(EncoderLayer):\n",
    "    def __init__(self, ch_in, ch_out, kernel_size, padding, pooling, dropout):\n",
    "        super(EncoderLayerBN, self).__init__(ch_in, ch_out, kernel_size, padding, pooling, dropout)\n",
    "\n",
    "        # your code goes here\n",
    "        self.block = nn.Sequential()\n",
    "\n",
    "\n",
    "class DecoderLayerBN(DecoderLayer):\n",
    "    def __init__(self, ch_in, ch_out, kernel_size, padding, dropout, skip_mode='concat', upsampling_mode='transpose', cropping=False):\n",
    "        super(DecoderLayerBN, self).__init__(ch_in, ch_out, kernel_size, padding, dropout, skip_mode, upsampling_mode, cropping)\n",
    "\n",
    "        if self.skip_mode == 'concat':\n",
    "            ch_hidden = ch_out + ch_out\n",
    "        elif self.skip_mode == 'add':\n",
    "            ch_hidden = ch_out\n",
    "        elif self.skip_mode == 'none':\n",
    "            ch_hidden = ch_out\n",
    "\n",
    "        # your code goes here\n",
    "        self.block = nn.Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training of the adapted U-Net\n",
    "\n",
    "#Network parameters\n",
    "input_channel = 1\n",
    "output_channel = 2\n",
    "hidden_dims = [32,64,128,256,512]\n",
    "kernel_size = 3                         \n",
    "padding_mode = # your code goes here\n",
    "skip_mode = 'concat'\n",
    "upsampling_mode = # your code goes here\n",
    "dropout = 0\n",
    "encoder_layer = EncoderLayerBN\n",
    "decoder_layer = DecoderLayerBN\n",
    "\n",
    "#Dataset parameters\n",
    "batch_size = 4\n",
    "patch_size_train = [256,256]\n",
    "stride_train = [128,128]       \n",
    "padding_train = [0,0]\n",
    "\n",
    "#Training parameters\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "eval_metric = Dice\n",
    "learning_rate = 1e-4\n",
    "\n",
    "\n",
    "#Preparing dataloader\n",
    "dataloader_train = get_dataloader(x=x_train, y=y_train, batch_size=batch_size, patch_size=patch_size_train, stride=stride_train, padding=padding_train, image_transforms=image_transforms_train, label_transforms=label_transforms_train, phase='train')\n",
    "dataloader_val = get_dataloader(x=x_val, y=y_val, batch_size=batch_size, patch_size=patch_size_train, stride=stride_train, padding=padding_train, image_transforms=image_transforms_val, label_transforms=label_transforms_val, phase='validate')\n",
    "\n",
    "#Initializing the U-Net\n",
    "unet = UNet2d(input_channel,output_channel,encoder_layer, decoder_layer, hidden_dims, kernel_size, padding_mode, skip_mode, upsampling_mode, dropout)\n",
    "optimizer = # your code goes here\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=20, verbose=True)\n",
    "\n",
    "trainer_adapted = UNetTrainer(unet, optimizer, lr_scheduler, loss_function, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "trainer_adapted.train(epochs, dataloader_train, dataloader_val)\n",
    "\n",
    "\n",
    "plot_results(\n",
    "    [trainer_original.training_losses, 'Training loss original U-Net'],\n",
    "    [trainer_adapted.training_losses, 'Training loss adapted U-Net']\n",
    ")\n",
    "\n",
    "plot_results(\n",
    "    [trainer_original.validation_losses, 'Validation loss original U-Net'],\n",
    "    [trainer_adapted.validation_losses, 'Validation loss adapted U-Net']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size_test = [256,256]\n",
    "stride_test = [128,128]\n",
    "padding_test = [0,0]\n",
    "\n",
    "test_scores_adapted = []\n",
    "\n",
    "for t in range(len(x_test)):\n",
    "    prediction, dice_score = predict(trainer_adapted.model, x_test[t], y_test[t], patch_size=patch_size_test, stride=stride_test, padding=padding_test, image_transforms = image_transforms_test, label_transforms=label_transforms_test, eval_metric = eval_metric)\n",
    "    test_scores_adapted.append(dice_score)\n",
    "\n",
    "print(f'Mean dice score for the test dataset using the original U-Net: {np.mean(test_scores_original)}')\n",
    "print(f'Mean dice score for the test dataset using the adapted U-Net: {np.mean(test_scores_adapted)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "prediction_original, dice_score_original = predict(trainer_original.model, x_test[idx], y_test[idx], patch_size=patch_size_original_test, stride=stride_original_test, padding=padding_original_test, image_transforms = image_transforms_test, label_transforms=label_transforms_test, eval_metric=eval_metric)\n",
    "prediction_adapted, dice_score_adapted = predict(trainer_adapted.model, x_test[idx], y_test[idx], patch_size=patch_size_test, stride=stride_test, padding=padding_test, image_transforms = image_transforms_test, label_transforms=label_transforms_test, eval_metric=eval_metric)\n",
    "\n",
    "plot_images(\n",
    "    [x_test[idx], 'Original Image'],\n",
    "    [prediction_original, f'Prediction original U-Net \\n Dice = {np.round(dice_score_original, 4)}'],\n",
    "    [prediction_adapted, f'Prediction adapted U-Net \\n Dice = {np.round(dice_score_adapted, 4)}'],\n",
    "    [y_test[idx]>0, 'Ground truth']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "In order to see the impact of data augmentation, train the same model without applying any augmentation techniques. What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "image_transforms_train_no_aug = Compose([])\n",
    "\n",
    "# your code goes here\n",
    "label_transforms_train_no_aug = Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training of the adapted U-Net without data augmentation\n",
    "\n",
    "#Network parameters\n",
    "input_channel = 1\n",
    "output_channel = 2\n",
    "hidden_dims = [32,64,128,256,512]\n",
    "kernel_size = 3                         \n",
    "padding_mode = 'same'\n",
    "skip_mode = 'concat'\n",
    "upsampling_mode = 'interpolate'\n",
    "dropout = 0\n",
    "encoder_layer = EncoderLayerBN          \n",
    "decoder_layer = DecoderLayerBN          \n",
    "\n",
    "#Dataset parameters\n",
    "batch_size = 4                          \n",
    "patch_size_train = [256,256]\n",
    "stride_train = [128,128]\n",
    "padding_train = [0,0]\n",
    "\n",
    "#Training parameters\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "eval_metric = Dice\n",
    "learning_rate = 1e-4\n",
    "\n",
    "\n",
    "#Preparing dataloader\n",
    "dataloader_train = get_dataloader(x=x_train, y=y_train, batch_size=batch_size, patch_size=patch_size_train, stride=stride_train, padding=padding_train, image_transforms=image_transforms_train_no_aug, label_transforms=label_transforms_train_no_aug, phase='train')\n",
    "dataloader_val = get_dataloader(x=x_val, y=y_val, batch_size=batch_size, patch_size=patch_size_train, stride=stride_train, padding=padding_train, image_transforms=image_transforms_val, label_transforms=label_transforms_val, phase='validate')\n",
    "\n",
    "#Initializing the U-Net\n",
    "unet = UNet2d(input_channel,output_channel,encoder_layer, decoder_layer, hidden_dims, kernel_size, padding_mode, skip_mode, upsampling_mode, dropout)\n",
    "optimizer = optim.Adam(unet.parameters(), lr = learning_rate)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=20, verbose=True)\n",
    "\n",
    "trainer_adapted_no_aug = UNetTrainer(unet, optimizer, lr_scheduler, loss_function, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "trainer_adapted_no_aug.train(epochs, dataloader_train, dataloader_val)\n",
    "\n",
    "\n",
    "plot_results(\n",
    "    [trainer_adapted.training_losses, 'Training loss adapted U-Net'],\n",
    "    [trainer_adapted_no_aug.training_losses, 'Training loss adapted U-Net w/o augmentation']\n",
    ")\n",
    "\n",
    "plot_results(\n",
    "    [trainer_adapted.validation_losses, 'Validation loss adapted U-Net'],\n",
    "    [trainer_adapted_no_aug.validation_losses, 'Validation loss adapted U-Net w/o augmentation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size_test = [256,256]\n",
    "stride_test = [128,128]\n",
    "padding_test = [0,0]\n",
    "\n",
    "test_scores_adapted_no_aug = []\n",
    "\n",
    "for t in range(len(x_test)):\n",
    "    prediction, dice_score = predict(trainer_adapted_no_aug.model, x_test[t], y_test[t], patch_size=patch_size_test, stride=stride_test, padding=padding_test, image_transforms = image_transforms_test, label_transforms=label_transforms_test, eval_metric=eval_metric)\n",
    "    test_scores_adapted_no_aug.append(dice_score)\n",
    "\n",
    "print(f'Mean dice score for the test dataset using the adapted U-Net with data augmentation: {np.mean(test_scores_adapted)}')\n",
    "print(f'Mean dice score for the test dataset using the adapted U-Net without augmentation: {np.mean(test_scores_adapted_no_aug)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "prediction_adapted, dice_score_adapted = predict(trainer_adapted.model, x_test[idx], y_test[idx], patch_size=patch_size_test, stride=stride_test, padding=padding_test, image_transforms = image_transforms_test, label_transforms=label_transforms_test, eval_metric=eval_metric)\n",
    "prediction_adapted_no_aug, dice_score_adapted_no_aug = predict(trainer_adapted_no_aug.model, x_test[idx], y_test[idx], patch_size=patch_size_test, stride=stride_test, padding=padding_test, image_transforms = image_transforms_test, label_transforms=label_transforms_test, eval_metric=eval_metric)\n",
    "\n",
    "plot_images(\n",
    "    [x_test[idx], 'Original image'],\n",
    "    [prediction_adapted, f'Prediction adapted U-Net \\n Dice = {np.round(dice_score_adapted, 4)}'],\n",
    "    [prediction_adapted_no_aug, f'Prediction adapted U-Net w/o augmentation \\n Dice = {np.round(dice_score_adapted_no_aug, 4)}'],\n",
    "    [y_test[idx]>0, 'Ground truth']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "In [Exercise 1](#exercise-1-u-net-decoder), besides concatenation you implemented two additional options for the skip-connections. Train a network with one of these options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training of the adapted U-Net without skip-connections\n",
    "\n",
    "#Network parameters\n",
    "input_channel = 1\n",
    "output_channel = 2\n",
    "hidden_dims = [32,64,128,256,512]\n",
    "kernel_size = 3                         \n",
    "padding_mode = 'same'                   \n",
    "skip_mode = # your code goes here                     \n",
    "upsampling_mode = 'interpolate'         \n",
    "dropout = 0                             \n",
    "encoder_layer = EncoderLayerBN          \n",
    "decoder_layer = DecoderLayerBN          \n",
    "\n",
    "#Dataset parameters\n",
    "batch_size = 4                          \n",
    "patch_size_train = [256,256]            \n",
    "stride_train = [128,128]                \n",
    "padding_train = [0,0]                   \n",
    "\n",
    "#Training parameters\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "eval_metric = Dice\n",
    "learning_rate = 1e-4\n",
    "\n",
    "\n",
    "#Preparing dataloader\n",
    "dataloader_train = get_dataloader(x=x_train, y=y_train, batch_size=batch_size, patch_size=patch_size_train, stride=stride_train, padding=padding_train, image_transforms=image_transforms_train, label_transforms=label_transforms_train, phase='train')\n",
    "dataloader_val = get_dataloader(x=x_val, y=y_val, batch_size=batch_size, patch_size=patch_size_train, stride=stride_train, padding=padding_train, image_transforms=image_transforms_val, label_transforms=label_transforms_val, phase='validate')\n",
    "\n",
    "#Initializing the U-Net\n",
    "unet = UNet2d(input_channel,output_channel,encoder_layer, decoder_layer, hidden_dims, kernel_size, padding_mode, skip_mode, upsampling_mode, dropout)\n",
    "optimizer = optim.Adam(unet.parameters(), lr = learning_rate)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, patience = 20, verbose = True)\n",
    "\n",
    "trainer_adapted_no_skip = UNetTrainer(unet, optimizer, lr_scheduler, loss_function, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "trainer_adapted_no_skip.train(epochs, dataloader_train, dataloader_val)\n",
    "\n",
    "\n",
    "plot_results(\n",
    "    [trainer_adapted.training_losses, 'Training loss adapted U-Net'],\n",
    "    [trainer_adapted_no_skip.training_losses, 'Training loss adapted U-Net w/o skip_connections']\n",
    ")\n",
    "\n",
    "plot_results(\n",
    "    [trainer_adapted.validation_losses, 'Validation loss adapted U-Net'],\n",
    "    [trainer_adapted_no_skip.validation_losses, 'Validation loss adapted U-Net w/o skip-connections']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size_test = [256,256]\n",
    "stride_test = [128,128]\n",
    "padding_test = [0,0]\n",
    "\n",
    "test_scores_adapted_no_skip = []\n",
    "\n",
    "for t in range(len(x_test)):\n",
    "    prediction, dice_score = predict(trainer_adapted_no_skip.model, x_test[t], y_test[t], patch_size=patch_size_test, stride=stride_test, padding=padding_test, image_transforms = image_transforms_test, label_transforms=label_transforms_test, eval_metric=eval_metric)\n",
    "    test_scores_adapted_no_skip.append(dice_score)\n",
    "\n",
    "print(f'Mean dice score for the test dataset using the adapted U-Net with concatenation skip-connections: {np.mean(test_scores_adapted)}')\n",
    "print(f'Mean dice score for the test dataset using the adapted U-Net with \"{skip_mode}\" skip-connections: {np.mean(test_scores_adapted_no_skip)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "prediction_adapted, eval_score_adapted = predict(trainer_adapted.model, x_test[idx], y_test[idx], patch_size=patch_size_test, stride=stride_test, padding=padding_test, image_transforms = image_transforms_test, label_transforms=label_transforms_test, eval_metric=eval_metric)\n",
    "prediction_adapted_no_skip, eval_score_adapted_no_skip = predict(trainer_adapted_no_skip.model, x_test[idx], y_test[idx], patch_size=patch_size_test, stride=stride_test, padding=padding_test, image_transforms = image_transforms_test, label_transforms=label_transforms_test, eval_metric=eval_metric)\n",
    "\n",
    "\n",
    "plot_images(\n",
    "    [x_test[idx], 'Original image'],\n",
    "    [prediction_adapted, f'Prediction adapted U-Net \\n Dice = {np.round(eval_score_adapted, 4)}'],\n",
    "    [prediction_adapted_no_skip, f'Prediction adapted U-Net with {skip_mode} skip-connections \\n Dice = {np.round(eval_score_adapted_no_skip, 4)}'],\n",
    "    [y_test[idx]>0, 'Ground truth']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
