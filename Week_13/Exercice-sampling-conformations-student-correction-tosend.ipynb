{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efbc9be0-482d-4783-8156-ba2b34922714",
   "metadata": {},
   "source": [
    "# Deep Architectures for Sampling Molecules\n",
    "\n",
    "You will use tools from Deeptime (https://deeptime-ml.github.io/) to implement a VAMPNET. You will then train a decoder to generate plausible conformational changes of molecules.\n",
    "\n",
    "The paper associated with the Deeptime package is: \n",
    "Hoffmann, M., Scherer, M.K., Hempel, T., Mardt, A., de Silva, B., Husic, B.E., Klus, S., Wu, H., Kutz, N., Brunton, S.L., & No√©, F. (2021). Deeptime: a Python library for machine learning dynamical models from time series data. Machine Learning: Science and Technology, 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddb8893-1fc3-4534-9cd2-f9ac892a3eb6",
   "metadata": {},
   "source": [
    "## Tutorial 1\n",
    "### Let us start by getting familiar with Deeptime\n",
    "\n",
    "To install Deeptime: `pip install deeptime` (see https://deeptime-ml.github.io/latest/index.html).\n",
    "\n",
    "Let us now generate a simple 2-D time series and extract a 1D compressed signal to learn more on how deeptime's VAMPNET works.\n",
    "\n",
    "Reference: https://deeptime-ml.github.io/latest/notebooks/vampnets.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294f2c6-b75a-4743-9d85-dd998981e5af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.set_num_threads(12)\n",
    "\n",
    "from deeptime.data import sqrt_model\n",
    "\n",
    "# The folowing generates samples of states and observations of a HMM (https://deeptime-ml.github.io/latest/api/generated/deeptime.data.sqrt_model.html)\n",
    "#The hidden variables takes two values 0 or 1 and oberved variable is a vector in 2D. \n",
    "\n",
    "dtraj, traj = sqrt_model(n_samples=10000)\n",
    "\n",
    "#The folowing is a plot of the time series\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, Y = np.meshgrid(\n",
    "    np.linspace(np.min(traj[:, 0]), np.max(traj[:, 0]), 100),\n",
    "    np.linspace(np.min(traj[:, 1]), np.max(traj[:, 1]), 100),\n",
    ")\n",
    "kde_input = np.dstack((X, Y)).reshape(-1, 2)\n",
    "\n",
    "kernel = stats.gaussian_kde(traj.T, bw_method=.1)\n",
    "Z = kernel(kde_input.T).reshape(X.shape)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "ax1.plot(dtraj[:500])\n",
    "ax1.set_title('Discrete trajectory')\n",
    "ax1.set_xlabel('time (a.u.)')\n",
    "ax1.set_ylabel('state')\n",
    "\n",
    "cm = ax2.contourf(X, Y, Z)\n",
    "plt.colorbar(cm, ax=ax2);\n",
    "ax2.set_title('Heatmap of observations');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ccf7ca-7103-496b-824e-3393dfa55329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to work with the VAMPNet / PyTorch API it can be convenient to first convert the trajectory into a time-lagged dataset.\n",
    "\n",
    "from deeptime.util.data import TrajectoryDataset\n",
    "\n",
    "dataset = TrajectoryDataset(1, traj.astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631a648e-3319-4568-a9bd-315ea32b6622",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val = int(len(dataset)*.3)\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, [len(dataset) - n_val, n_val])\n",
    "\n",
    "\n",
    "#The neural network \n",
    "from deeptime.util.torch import MLP\n",
    "lobe = MLP(units=[traj.shape[1], 15, 10, 10, 5, 1], nonlinearity=nn.ReLU)\n",
    "print(lobe)\n",
    "\n",
    "lobe = lobe.to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b6d79-1ce9-459a-8ca7-3aa7cc9d12e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptime.decomposition.deep import VAMPNet\n",
    "\n",
    "vampnet = VAMPNet(lobe=lobe, learning_rate=1e-4, device=device)\n",
    "\n",
    "# Dataset train valid\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader_train = DataLoader(train_data, batch_size=512, shuffle=True)\n",
    "loader_val = DataLoader(val_data, batch_size=len(val_data), shuffle=False)\n",
    "\n",
    "\n",
    "#Train\n",
    "from tqdm.notebook import tqdm\n",
    "model = vampnet.fit(loader_train, n_epochs=160,\n",
    "                    validation_loader=loader_val, progress=tqdm).fetch_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86198ede-54e0-43a8-9347-1bdc0b30a356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot loss\n",
    "plt.loglog(*vampnet.train_scores.T, label='training')\n",
    "plt.loglog(*vampnet.validation_scores.T, label='validation')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('score')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e62a04-7346-40f8-8ebc-48840e25e698",
   "metadata": {},
   "source": [
    "## Tutorial 2\n",
    "### Let us see how to learn an MSM\n",
    "Taken from https://deeptime-ml.github.io/latest/notebooks/mlmsm.html\n",
    "\n",
    "For a discrete Markov chain $\\pi(y \\mid x)$, with a finite state space $\\Omega$ and finite time steps, the likelihood of a sample trajectory $(x_t, t \\leq T)$ is given by,\n",
    "$$L(\\pi)= \\prod_{x,y \\in \\Omega} \\pi(y \\mid x)^{C_{y,x}}$$\n",
    "\n",
    "where $C_{y,x}$ is the number of times the transition $x \\to y$ happened. One can derive the maximum likelihood estimate analytically. This is what the following function MaximumLikelihoodMSM will be used for.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40f0ab-62f6-48a7-92af-d336671da673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptime.markov as markov\n",
    "\n",
    "estimator = markov.msm.MaximumLikelihoodMSM(\n",
    "    reversible=True,\n",
    "    stationary_distribution_constraint=None\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b2456-e03c-40ca-9c9b-fab1f778dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generting a Markov chain with three states\n",
    "import numpy as np\n",
    "\n",
    "p11 = 0.97\n",
    "p22 = 0.97\n",
    "p33 = 0.97\n",
    "P = np.array([[p11, 1 - p11, 0], [.5*(1 - p22), p22, 0.5*(1-p22)], [0, 1-p33, p33]])\n",
    "true_msm = markov.msm.MarkovStateModel(P)\n",
    "\n",
    "# Generating samples of that Markov chain\n",
    "trajectory = true_msm.simulate(50000)\n",
    "\n",
    "#Counting the trnaistions\n",
    "counts_estimator = markov.TransitionCountEstimator(\n",
    "    lagtime=1, count_mode=\"sliding\"\n",
    ")\n",
    "counts = counts_estimator.fit(trajectory).fetch_model()\n",
    "\n",
    "# The MLE MSM\n",
    "msm = estimator.fit(counts).fetch_model()\n",
    "\n",
    "#Printing the learned MSM\n",
    "print(\"Estimated transition matrix:\", msm.transition_matrix)\n",
    "print(\"Estimated stationary distribution:\", msm.stationary_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d86ab4-e0cd-4a05-ae0b-936bf53a76b1",
   "metadata": {},
   "source": [
    "## Exercice \n",
    "### VAMPNerts for Alanine Dipeptide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706cb492-b3a5-4695-a971-fe1b9b610a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install mdshare\n",
    "import mdshare\n",
    "\n",
    "#The time series of Alanine Dipeptide \n",
    "ala_coords_file = mdshare.fetch(\"alanine-dipeptide-3x250ns-heavy-atom-positions.npz\", working_directory=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499fe69e-acbf-4146-919a-a9eb493d421e",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Learn a VAMPNET on the time series of Alanine Dipeptide. The variable for this VAMPNET will be called \"model.\"\n",
    "\n",
    "Hint: See [this example](https://deeptime-ml.github.io/latest/notebooks/examples/ala2-example.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77696aa0-fddc-4665-818f-ab6008de138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correction \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import mdshare  # for trajectory data\n",
    "from deeptime.util.torch import MLP\n",
    "from tqdm.notebook import tqdm  # progress bar\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.set_num_threads(12)\n",
    "\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "with np.load(ala_coords_file) as fh:\n",
    "    data = [fh[f\"arr_{i}\"].astype(np.float32) for i in range(3)]\n",
    "\n",
    "from deeptime.util.data import TrajectoryDataset, TrajectoriesDataset\n",
    "\n",
    "dataset = TrajectoriesDataset.from_numpy(1, data)\n",
    "\n",
    "n_val = int(len(dataset)*.1)\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, [len(dataset) - n_val, n_val])\n",
    "\n",
    "lobe = nn.Sequential(\n",
    "    nn.BatchNorm1d(data[0].shape[1]),\n",
    "    nn.Linear(data[0].shape[1], 20), nn.ELU(),\n",
    "    nn.Linear(20, 20), nn.ELU(),\n",
    "    nn.Linear(20, 20), nn.ELU(),\n",
    "    nn.Linear(20, 20), nn.ELU(),\n",
    "    nn.Linear(20, 20), nn.ELU(),\n",
    "    nn.Linear(20, 6),\n",
    "    nn.Softmax(dim=1)  # obtain fuzzy probability distribution over output states\n",
    ")\n",
    "\n",
    "lobe = lobe.to(device=device)\n",
    "print(lobe)\n",
    "\n",
    "from deeptime.decomposition.deep import VAMPNet\n",
    "\n",
    "vampnet = VAMPNet(lobe=lobe, learning_rate=5e-3, device=device)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader_train = DataLoader(train_data, batch_size=10000, shuffle=True)\n",
    "loader_val = DataLoader(val_data, batch_size=len(val_data), shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e2b94-d522-466a-be14-b04813d8a0e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nepochs=30\n",
    "model = vampnet.fit(loader_train, n_epochs=nepochs,\n",
    "                    validation_loader=loader_val, progress=tqdm).fetch_model()\n",
    "plt.loglog(*vampnet.train_scores.T, label='training')\n",
    "plt.loglog(*vampnet.validation_scores.T, label='validation')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('score')\n",
    "plt.legend();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685bd4ed-f442-4641-8931-d1daf7370581",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "The VAMPNET compresses the time series, it plays the role of an encoder; the output of the VAMPNET plays the role of a 'latent space.'\n",
    "\n",
    "2.1) Cluster the compressed time series dataset.\n",
    "\n",
    "Hint: The compressed dataset is the collection of compressed configurations $model(x_t)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08af8b-1545-40fd-a428-16ca59ac5db8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Correction \n",
    "from deeptime.clustering import KMeans\n",
    "\n",
    "\n",
    "\n",
    "projections = [model(traj) for traj in data]\n",
    "cluster = KMeans(4, progress=tqdm).fit_fetch(projections)\n",
    "dtrajs = [cluster.transform(x) for x in projections]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c0d12-ca75-4138-b26a-79f45d8f200c",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "3.1) Learn an MSM from the clustering on the compressed dataset.\n",
    "\n",
    "3.2) Generate samples from this learned discrete Markov chain.\n",
    "\n",
    "Hint: Refer to tutorial 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ae88f-da02-47fb-86e5-e80cf0506736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptime.markov import TransitionCountEstimator\n",
    "import deeptime.markov as markov\n",
    "estimator = markov.msm.MaximumLikelihoodMSM(\n",
    "    reversible=True,\n",
    "    stationary_distribution_constraint=None\n",
    ")\n",
    "\n",
    "\n",
    "counts_estimator = markov.TransitionCountEstimator(\n",
    "    lagtime=1, count_mode=\"sliding\"\n",
    ")\n",
    "counts = counts_estimator.fit(dtrajs).fetch_model()\n",
    "\n",
    "# The MLE MSM\n",
    "msm = estimator.fit(counts).fetch_model()\n",
    "\n",
    "import random\n",
    "\n",
    "#generating novel trajectories\n",
    "matrix=msm.transition_matrix\n",
    "sample=[0]\n",
    "state=0\n",
    "for i in range(100):\n",
    "    rand=random.uniform(0, 1)\n",
    "    count=0\n",
    "    stp=1\n",
    "    for j in range(matrix.shape[1]):\n",
    "        if count+matrix[state][j]>rand and stp:\n",
    "            state=j\n",
    "            stp=0\n",
    "        count+=matrix[state][j]\n",
    "    sample.append(state)\n",
    "    \n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73ed913-a537-4c14-9527-38cbb3475f14",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "4.1) Freeze the weights of the encoder and learn a decoder to generate configurations of Alanine Dipeptide from the clusters.\n",
    "\n",
    "4.2) Generate samples of the time evolution of Alanine Dipeptide from this learned discrete Markov chain.\n",
    "\n",
    "Hint: Freeze the weights of the VAMPNET, define a neural network that will serve as the decoder. The full network is Decoder + Encoder with frozen weights. Train the autoencoder with the Mean Square Error and use gradient descent for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe88c4-6999-4c8f-a664-d9c36619b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=model.lobe\n",
    "shape_in=cluster.cluster_centers.shape[1]\n",
    "\n",
    "#For example:\n",
    "decoder = MLP(units=[shape_in, 10, 15, 15, data[0].shape[1]], nonlinearity=nn.ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45148976-3835-40c1-855c-7506e756fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(self.encoder(x))\n",
    "        return x    \n",
    "autoenc=AutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c9e6d-7a68-4008-8865-c7e2e1a0dcc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(train_data, batch_size=30, shuffle=True)\n",
    "optimizer = torch.optim.SGD(autoenc.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = torch.nn.MSELoss()\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, datas in enumerate(training_loader, 0):\n",
    "\n",
    "        inputs, input_lag = datas #only keep inputs without lag\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs =autoenc(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
